version: '3.8'

# Voice Cloning TTS Server (XTTS-v2)
#
# This service provides a Text-to-Speech API using Coqui TTS (XTTS-v2).
# It is designed to run on a Linux server with NVIDIA GPU support.
#
# API Endpoints:
#   GET  /health            - Check server status and GPU availability
#   GET  /voices            - List available voice reference files
#   POST /synthesize        - Generate speech (returns base64 audio)
#   POST /synthesize/raw    - Generate speech (returns raw binary audio)
#
# Usage:
#   docker compose up -d
#   docker compose logs -f voice-tts

services:
  voice-tts:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: voice-tts-server
    restart: unless-stopped
    
    # GPU Configuration
    # Requires NVIDIA Container Toolkit to be installed on the host.
    # Checks for 1 NVIDIA GPU with 'gpu' capabilities.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Port mapping
    # Maps host port 8080 to container port 8080
    # Access API at http://localhost:8080 or http://<server-ip>:8080
    ports:
      - "8080:8080"
    
    # Volume mounts for persistent data
    volumes:
      # Voice reference audio files
      # Place .wav files (approx 15-30s) here to be used as voice clones.
      # The server will scan this directory for available voices.
      - ./voice_references:/app/voice_references
      
      # Generated audio output
      # Synthesized speech files are saved here.
      - ./audio_output:/app/audio_output
      
      # Server logs
      # Persists application logs for debugging.
      - ./logs:/app/logs
      
      # Model cache
      # Persists downloaded XTTS-v2 model weights (~2-3GB) to avoid
      # re-downloading on every restart.
      - ./models:/app/models
      
      # Training data (for fine-tuning)
      # Mounts the training data directory for finetune_xtts.py
      # - ./training_data:/app/training_data
    
    # Environment variables
    environment:
      # Expose all NVIDIA GPUs to the container
      - NVIDIA_VISIBLE_DEVICES=all
      
      # Directory where TTS models are downloaded/cached (matches volume mount)
      - TTS_HOME=/app/models
      
      # Automatically agree to Coqui TTS Terms of Service
      - COQUI_TOS_AGREED=1
      
      # Force Python stdout/stderr to be unbuffered (logs show up immediately)
      - PYTHONUNBUFFERED=1
    
    # Logging configuration
    # Limits log file size to prevent disk filling
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    
    # Network settings
    networks:
      - tts-network

networks:
  tts-network:
    driver: bridge
